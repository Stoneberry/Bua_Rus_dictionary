{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEI BUA PARSER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Подготовили - Анастасия Костяницына, Артем Копецкий\n",
    "\n",
    "* Парсинг параллельного бурятского корпуса. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "from string import punctuation\n",
    "import time\n",
    "from tqdm import tqdm_notebook \n",
    "from collections import defaultdict, Counter\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4 import Tag\n",
    "import copy\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Структура данных для словарной статьи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Entry:\n",
    "    \n",
    "    def __init__(self, lex):\n",
    "        self.lex = lex\n",
    "        \n",
    "        self.pos = {}\n",
    "        self.sem = {}\n",
    "        self.gramm = {}\n",
    "        self.examples = defaultdict(list)\n",
    "        \n",
    "    def add_pos(self, pos):\n",
    "        if pos not in self.pos:\n",
    "            self.pos[pos] = {}\n",
    "    \n",
    "    def add_sem(self, pos, sem):\n",
    "        if sem not in self.pos[pos]:\n",
    "            self.pos[pos][sem] = {}\n",
    "    \n",
    "    def add_form(self, pos, sem, gramm, form):\n",
    "        if gramm not in self.pos[pos][sem]:\n",
    "            self.pos[pos][sem][gramm] = form\n",
    "            \n",
    "    def add_example(self, sem, doc, pair):\n",
    "        self.examples[sem].append((doc, pair))\n",
    "            \n",
    "    def get_tree(self):\n",
    "        return self.pos\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Структура данных для примеров (текст на бурятском, текст на русском)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pair:\n",
    "    \n",
    "    def __init__(self, id_, bua, rus, text_info):\n",
    "        self.id_ = id_\n",
    "        self.tokens_bua, self.infos_bua = bua\n",
    "        self.tokens_rus, self.infos_rus = rus\n",
    "        \n",
    "        self.text_bua = ''.join(self.tokens_bua).strip()\n",
    "        self.text_rus = ''.join(self.tokens_rus).strip()\n",
    "        self.text_info = text_info\n",
    "        \n",
    "    def find_rus_in_bua(self, word):\n",
    "        res = []\n",
    "        \n",
    "        for token, info in zip(self.tokens_bua, self.infos_bua):\n",
    "            _info = info.get('sem')\n",
    "            \n",
    "            if _info is not None and word in _info:\n",
    "                res.append((word, token, info))\n",
    "                \n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lemma(cur_soup, unique_entry):\n",
    "    \n",
    "    '''\n",
    "    Добавляет лемму в шаблон\n",
    "    '''\n",
    "\n",
    "    cur_soup.metalemma.string = unique_entry\n",
    "    cur_soup.orth.string = unique_entry\n",
    "    cur_soup.orth['main'] = \"True\"\n",
    "    \n",
    "    \n",
    "def create_entry():\n",
    "    \n",
    "    '''\n",
    "    Если слово может иметь несколько частей речи, они оформляются как hom.\n",
    "    Функуия сосздает дополнительные entry\n",
    "    '''\n",
    "    \n",
    "    ex = '<entry><form><orth></orth></form><gramGrp></gramGrp></entry>'\n",
    "    soup = BeautifulSoup(ex, 'lxml')\n",
    "    soup.entry['type'] = 'hom'\n",
    "\n",
    "    return soup\n",
    "\n",
    "\n",
    "def add_tag(cur_soup, form_soup, name, value):\n",
    "    \n",
    "    '''новый тег'''\n",
    "    \n",
    "    new_tag = cur_soup.new_tag(name)\n",
    "    new_tag.string = value\n",
    "    \n",
    "    if new_tag not in form_soup:\n",
    "        form_soup.append(new_tag)\n",
    "\n",
    "        \n",
    "def add_POS(cur_soup, entr_soup, pos): \n",
    "    '''\n",
    "    Добавляем часть речи\n",
    "    '''\n",
    "    \n",
    "    add_tag(cur_soup, entr_soup.gramgrp, 'pos', pos)\n",
    "    \n",
    "    \n",
    "def gram_info(info, cur_soup, form_soup):\n",
    "    \n",
    "    '''\n",
    "    <pos> ч.речи </pos>\n",
    "    <gender>род</gender>\n",
    "    <num>число</num>\"\n",
    "    <per>лицо</per>\n",
    "    <animat> одушевленность/неодушевленность </animat>\n",
    "    <declen> склонение (или нескл.) </declen>\n",
    "    <mood>наклонение</mood>\n",
    "    <asp> вид (совершенный/несовершенный) </asp>\n",
    "    <transit> переходный/непереходный </transit>\n",
    "    <refl> возвратность </refl>\n",
    "    <pron_type> личное/неличное </pron_type>\n",
    "    <gov> для предлогов и глаголов: если указано, с какими падежами используется. информация об управлении. </gov>\n",
    "    <iType> inflectional class</iType>\n",
    "    <note> любая грам. информация, которая не поместилась в тэги выше</note>\n",
    "    '''\n",
    "    \n",
    "    if info.isupper():\n",
    "        add_tag(cur_soup, form_soup, 'pos', info)\n",
    "        \n",
    "    elif info in gender:\n",
    "        add_tag(cur_soup, form_soup, 'gender', info)\n",
    "        \n",
    "    elif info in num:\n",
    "        add_tag(cur_soup, form_soup, 'num', info)\n",
    "        \n",
    "    elif info in per:\n",
    "        add_tag(cur_soup, form_soup, 'per', info)  \n",
    "        \n",
    "    elif info in aspect:\n",
    "        add_tag(cur_soup, form_soup, 'asp', info)\n",
    "    \n",
    "    elif info in anim:\n",
    "        add_tag(cur_soup, form_soup, 'animat', info)\n",
    "    \n",
    "    else:\n",
    "        add_tag(cur_soup, form_soup, 'note', info)\n",
    "        \n",
    "        \n",
    "def add_gram(cur_soup, entr_soup, gram, typ='entry'):\n",
    "    \"\"\"\n",
    "     добавляем грам инфу в энтри\n",
    "    \"\"\"\n",
    "    \n",
    "    if typ == 'entry':  # добавляем грам инфу в энтри\n",
    "        domain = entr_soup.gramgrp\n",
    "        \n",
    "    if typ == 'sense': 1  # добавляем грам инфу в сенс \n",
    "    \n",
    "        \n",
    "    for gr in gram.split(' '):\n",
    "        gram_info(gr, cur_soup, domain)\n",
    "    \n",
    "\n",
    "def add_form_inflected(cur_soup, entr_soup, word, gram_info):\n",
    "    \n",
    "    '''\n",
    "    добавляет формы слова\n",
    "    \n",
    "    <form type=\"inflected\">\n",
    "        <case>...</case>\n",
    "        <num>число</num>\"\n",
    "        <tns>...</tns>\n",
    "        <orth>...</orth>\n",
    "     </form>\n",
    "    '''\n",
    "    \n",
    "    xml_form = '<form type=\"inflected\"><orth></orth></form>'\n",
    "    \n",
    "    form_soup = BeautifulSoup(copy.copy(xml_form), 'lxml')\n",
    "    form_soup.orth.string = word\n",
    "    \n",
    "    for i in gram_info.split(' '):\n",
    "        \n",
    "        if i in cases:\n",
    "            add_tag(cur_soup, form_soup.form, 'case', i)\n",
    "        elif i in num:\n",
    "            add_tag(cur_soup, form_soup.form, 'num', i)\n",
    "        elif i in tense:\n",
    "            add_tag(cur_soup, form_soup.form, 'tns', i)\n",
    "        elif i in v_form:\n",
    "            add_tag(cur_soup, form_soup.form, 'type', i)\n",
    "        else:\n",
    "            add_tag(cur_soup, form_soup.form, 'note', i)\n",
    "        \n",
    "    entr_soup.entry.append(form_soup.form)\n",
    "        \n",
    "\n",
    "def add_text_info(bua_soup, txt_info):\n",
    "    \n",
    "    ''' \n",
    "    <source>\n",
    "        <author> Перечислены авторы </author>\n",
    "        <translator>...</translator>\n",
    "        <pubdate>...</pubdate>\n",
    "        <title>Название</title>\n",
    "    <source>\n",
    "    '''\n",
    "    \n",
    "    xml_source = '<source><title></title><author></author><translator></translator><pubdate></pubdate></source>'\n",
    "    sour_soup = BeautifulSoup(copy.copy(xml_source), 'lxml')\n",
    "    \n",
    "    sour_soup.author.string = txt_info['author']\n",
    "    sour_soup.translator.string = txt_info['translator']\n",
    "    sour_soup.pubdate.string = txt_info['created']\n",
    "    sour_soup.title.string = txt_info['header']\n",
    "    \n",
    "    bua_soup.cit.append(sour_soup.source)\n",
    "    \n",
    "\n",
    "def add_sense(cur_soup, entr_soup, sns, idx, examples):\n",
    "    \n",
    "    xml_sense = '<sense n=\"{}\"></sense>'.format(str(idx + 1))\n",
    "    xml_bua = '<cit type=\"example\"><quote></quote></cit>'\n",
    "    xml_rus = '<cit type=\"translation\" lang_code=\"rus\"><quote></quote></cit>'\n",
    "    \n",
    "    sense_soup = BeautifulSoup(copy.copy(xml_sense), 'lxml')\n",
    "    \n",
    "    new_def = cur_soup.new_tag('def')\n",
    "    new_text = cur_soup.new_tag('text')\n",
    "    new_text.string = sns\n",
    "    new_def.append(new_text)\n",
    "    sense_soup.sense.append(new_def)\n",
    "    \n",
    "    for exampl in examples:\n",
    "        \n",
    "        text_info = exampl[0]\n",
    "        text_pairs = exampl[1]\n",
    "\n",
    "        rus_soup = BeautifulSoup(copy.copy(xml_rus), 'lxml')\n",
    "        rus_soup.quote.string = text_pairs.text_rus\n",
    "        \n",
    "        bua_soup = BeautifulSoup(copy.copy(xml_bua), 'lxml')\n",
    "        bua_soup.quote.string = text_pairs.text_bua\n",
    "        bua_soup.cit.append(rus_soup.cit)\n",
    "        \n",
    "        add_text_info(bua_soup, text_info)\n",
    "        \n",
    "        sense_soup.sense.append(bua_soup.cit)\n",
    "    \n",
    "    entr_soup.entry.append(sense_soup.sense)\n",
    "    \n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tei_parser(word):\n",
    "    \n",
    "    '''\n",
    "    Парсинг\n",
    "    '''\n",
    "    \n",
    "    sns = 1\n",
    "    \n",
    "    cur_soup = BeautifulSoup(copy.copy(xml_n), 'lxml')\n",
    "    add_lemma(cur_soup, word.lex)\n",
    "    cur_soup.entry['type'] = 'main'\n",
    "    \n",
    "    tree = word.get_tree()\n",
    "    examples = word.examples\n",
    "\n",
    "    for index, POS in enumerate(tree): \n",
    "        \n",
    "        if index == 0: entr_soup = cur_soup\n",
    "        else: \n",
    "            entr_soup = create_entry()\n",
    "            entr_soup.orth.string = word.lex\n",
    "        \n",
    "        add_POS(cur_soup, entr_soup, POS)\n",
    "# --------------------------------------------------------------  gram_info\n",
    "        \n",
    "        for idx, sense in enumerate(tree[POS]):\n",
    "            \n",
    "            gram_info = tree[POS][sense]\n",
    "            \n",
    "            for gram in gram_info:\n",
    "                \n",
    "                if 'nom' in gram:  # добавляем грам. инфу к лемме существительного\n",
    "                    add_gram(cur_soup, entr_soup, gram, typ='entry')\n",
    "                \n",
    "                elif 'inf' in gram:  # добавляем грам. инфу к лемме глагола\n",
    "                    add_gram(cur_soup, entr_soup, gram, typ='entry')\n",
    "        \n",
    "                elif gram_info[gram] != word.lex:  # добавляем формы слова\n",
    "                    add_form_inflected(cur_soup, entr_soup, gram_info[gram], gram)\n",
    "\n",
    "# --------------------------------------------------------------  sense     \n",
    " \n",
    "            add_sense(cur_soup, entr_soup, sense, idx, examples[sense])\n",
    "        \n",
    "        \n",
    "        if index != 0:\n",
    "            cur_soup.superentry.append(entr_soup.entry)\n",
    "        \n",
    "    return cur_soup.superentry\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Грам. теги, взятые из НКРЯ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases = ['nom', 'gen', 'dat', 'dat2', 'acc', 'ins', 'loc', 'gen2', 'acc2', 'loc2', 'voc', 'adnum', 'comit']\n",
    "num = ['pl', 'sg']\n",
    "gender = ['m', 'f', 'm-f', 'n']\n",
    "anim = ['anim', 'inan']\n",
    "aspect = ['pf', 'ipf']\n",
    "tense = ['praet', 'praes', 'fut']\n",
    "per = ['1p', '2p', '3p']\n",
    "v_form = ['inf', 'partcp', 'ger']\n",
    "\n",
    "\n",
    "xml_n = '<superEntry><metalemma></metalemma><entry><form><orth></orth></form><gramGrp></gramGrp></entry></superEntry>'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скаченные, подготовленные (Entry) данные для tei "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/bua_rus_entries.pkl', 'rb') as f:\n",
    "     data_new = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/bua_rus_entries2.pkl', 'rb') as f:\n",
    "     data_new2 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Итог:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_tei(data, cur_soup):\n",
    "    \n",
    "    '''\n",
    "    Объединяем все в один файл\n",
    "    '''\n",
    "    \n",
    "    ent_count = 0\n",
    "    \n",
    "    for i in data_new:\n",
    "        x = data_new[i]\n",
    "        \n",
    "        if x != {}:\n",
    "            for word in x:\n",
    "                ent_count += 1\n",
    "                Entry = x[word]\n",
    "                word_tei = tei_parser(Entry)\n",
    "                cur_soup.div.append(word_tei)\n",
    "\n",
    "    return cur_soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_source = '<xml><fileDesc><respStmt><name>Артем Копецкий, Анастасия Костяницына</name></respStmt><extent>{}</extent><sourceDesc><ref target=\"http://www.ruscorpora.ru/saas/search-para-bua.html\">НКРЯ</ref><p>Параллельный корпус бурятского языка из НКРЯ.</p></sourceDesc></fileDesc><front><head><title volume=\"\" dict_id=\"\">Параллельный корпус бурятского языка.</title></head><dict_lang><language n=\"source\">\"bua\"</language><language n=\"target\">\"rus\"</language><language n=\"example\"></language></dict_lang></front><body><div></div></body></xml>'\n",
    "cur_soup = BeautifulSoup(copy.copy(xml_source), 'lxml')\n",
    "\n",
    "cur_soup = dict_tei(data_new, cur_soup)\n",
    "cur_soup = dict_tei(data_new2, cur_soup)\n",
    "\n",
    "tei = cur_soup.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bua_parsed.tei', 'a', encoding='utf-8') as gr:\n",
    "    gr.write(tei.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bua_parsed.txt', 'a', encoding='utf-8') as gr:\n",
    "    gr.write(tei.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вспомогательные "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# rus_word : { bua_word : [tree, examples]\n",
    "# examples -> { sense : [text_info, text_rus, text_bua]\n",
    "\n",
    "\n",
    "d = defaultdict(dict)\n",
    "\n",
    "def dict_words(data_new, d):\n",
    "    \n",
    "    '''\n",
    "    paw ругается на наши структуры данных, поэтому преобразуем все в словарь для сайта \n",
    "    '''\n",
    "    \n",
    "    for word in data_new:\n",
    "        if data_new[word] != {}:\n",
    "            b = defaultdict(list)\n",
    "        \n",
    "            for bu_w in data_new[word]:\n",
    "            \n",
    "                entry = data_new[word][bu_w]\n",
    "                tree = entry.get_tree()\n",
    "                b[bu_w].append(tree)\n",
    "                c = defaultdict(dict)\n",
    "            \n",
    "                for sns in entry.examples:\n",
    "                    a = []\n",
    "            \n",
    "                    for ex in entry.examples[sns]:\n",
    "                        a.append((ex[0], ex[1].text_rus, ex[1].text_bua))\n",
    "                    \n",
    "                    c[sns] = a\n",
    "                b[bu_w].append(c)\n",
    "            d[word] = b\n",
    "            \n",
    "    return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dict_words(data_new, d)\n",
    "d = dict_words(data_new2, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/dict_bua_rus.pkl', 'wb') as f:\n",
    "     pickle.dump(d, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus_d = defaultdict(set)\n",
    "\n",
    "\n",
    "def rus_rus(data_new, d):\n",
    "    \n",
    "    '''\n",
    "    чтобы, когда вводились русские слова в поисковик, находились совпадения и со случаями, когда в определении несколько слов, а не только наше из запроса\n",
    "    '''\n",
    "\n",
    "    for i in data_new:\n",
    "    \n",
    "        for word in data_new[i]:\n",
    "        \n",
    "            a = data_new[i][word].get_tree()\n",
    "        \n",
    "            for pos in a:\n",
    "            \n",
    "                for sns in a[pos]:\n",
    "                    snss = sns.split(',')\n",
    "                    for l in snss:\n",
    "                        if l.startswith(' '):\n",
    "                            l = l[1:]\n",
    "                        d[l].add(i)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "rus_d = rus_rus(data_new, rus_d )\n",
    "rus_d = rus_rus(data_new2, rus_d )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/rus_rus2.pkl', 'wb') as f:\n",
    "     pickle.dump(rus_d, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы можно было удобно искать по бурятскому слову"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "bua_data = defaultdict(list)\n",
    "\n",
    "def bua_data_p(data_new, bua_data):\n",
    "\n",
    "    for ru_word in data_new:\n",
    "\n",
    "        data = data_new.get(ru_word)\n",
    "    \n",
    "        if data and data != {}:\n",
    "        \n",
    "            for bua_word in data:\n",
    "                bua_data[bua_word] = data[bua_word]#.get_tree()\n",
    "    \n",
    "    return bua_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "bua_data = bua_data_p(d, bua_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/bua_data2.pickle', 'wb') as f:\n",
    "     pickle.dump(bua_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
